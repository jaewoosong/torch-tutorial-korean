{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN from Andrej Karpathy\n",
    "(http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "This file trains a character-level multi-layer RNN on text data\n",
    "\n",
    "Code is based on implementation in \n",
    "https://github.com/oxford-cs-ml-2015/practical6\n",
    "but modified to have multi-layer support, GPU support, as well as\n",
    "many other common model/optimization bells and whistles.\n",
    "The practical6 code is in turn based on \n",
    "https://github.com/wojciechz/learning_to_execute\n",
    "which is turn based on other stuff in Torch, etc... (long lineage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "require 'crnn'\n",
    "\n",
    "CharSplitLMMinibatchLoader = require 'crnn.util.CharSplitLMMinibatchLoader'\n",
    "model_utils = require 'crnn.util.model_utils'\n",
    "LSTM = require 'crnn.model.LSTM'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a character-level language model\n",
    "Create a table of options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "    -- data directory. Should contain the file input.txt with input data\n",
    "    data_dir = 'crnn_data/tinyshakespeare',\n",
    "\n",
    "    -- model parameters\n",
    "\n",
    "    -- size of LSTM internal state\n",
    "    rnn_size = 128,\n",
    "    -- number of layers in the LSTM\n",
    "    num_layers = 2,\n",
    "    -- for now only lstm is supported. keep fixed\n",
    "    model = 'lstm',\n",
    "\n",
    "    -- optimization parameters\n",
    "\n",
    "    -- learning rate\n",
    "    learning_rate = 2e-3,\n",
    "    -- learning rate decay\n",
    "    learning_rate_decay = 0.97,\n",
    "    -- in number of epochs, when to start decaying the learning rate\n",
    "    learning_rate_decay_after = 10,\n",
    "    -- decay rate for rmsprop\n",
    "    decay_rate = 0.95,\n",
    "    -- dropout to use just before classifier. 0 = no dropout\n",
    "    dropout = 0,\n",
    "    -- number of timesteps to unroll for\n",
    "    seq_length = 50,\n",
    "    -- number of sequences to train on in parallel\n",
    "    batch_size = 50,\n",
    "    -- number of full passes through the training data\n",
    "    max_epochs = 30,\n",
    "    -- clip gradients at\n",
    "    grad_clip = 5,\n",
    "    -- fraction of data that goes into train set\n",
    "    train_frac = 0.95,\n",
    "    -- fraction of data that goes into validation set\n",
    "    val_frac = 0.05,\n",
    "\n",
    "    -- bookkeeping\n",
    "\n",
    "    -- torch manual random number generator seed\n",
    "    seed = 123,\n",
    "    -- how many steps/minibatches between printing out the loss\n",
    "    print_every = 1,\n",
    "    -- every how many iterations should we evaluate on validation data?\n",
    "    eval_val_every = 1000,\n",
    "    -- output directory where checkpoints get written\n",
    "    checkpoint_dir = 'cv',\n",
    "    -- filename to autosave the checkpont to. Will be inside checkpoint_dir/\n",
    "    savefile = 'lstm',\n",
    "\n",
    "    -- GPU/CPU\n",
    "\n",
    "    -- which gpu to use. -1 = use CPU\n",
    "    gpuid = -1,\n",
    "}\n",
    "torch.setnumthreads(2)\n",
    "torch.manualSeed(opt.seed)\n",
    "-- train / val / test split for data, in fractions\n",
    "test_frac = math.max(0, 1 - opt.train_frac - opt.val_frac)\n",
    "split_sizes = {opt.train_frac, opt.val_frac, test_frac}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be split as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(string.format('Train Split = %g\\nValidation Split = %g\\nTest Split = %g\\n',unpack(split_sizes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- create the data loader class\n",
    "loader = CharSplitLMMinibatchLoader.create(opt.data_dir, opt.batch_size, opt.seq_length, split_sizes)\n",
    "\n",
    "-- the number of distinct characters\n",
    "vocab_size = loader.vocab_size\n",
    "\n",
    "print('vocab size: ' .. vocab_size)\n",
    "\n",
    "-- make sure output directory exists\n",
    "if not path.exists(opt.checkpoint_dir) then\n",
    "    lfs.mkdir(opt.checkpoint_dir)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "An RNN is created by first creating a single time-step model and then by cloning that in time.\n",
    "Here we create the core model that will be later replicated many times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "-- define the model: prototypes for one timestep, then clone them in time\n",
    "protos = {}\n",
    "print('creating an LSTM with ' .. opt.num_layers .. ' layers')\n",
    "protos.rnn = LSTM.lstm(vocab_size, opt.rnn_size, opt.num_layers, opt.dropout)\n",
    "-- the initial state of the cell/hidden states\n",
    "init_state = {}\n",
    "for L=1,opt.num_layers do\n",
    "    local h_init = torch.zeros(opt.batch_size, opt.rnn_size)\n",
    "    if opt.gpuid >=0 then h_init = h_init:cuda() end\n",
    "    table.insert(init_state, h_init:clone())\n",
    "    table.insert(init_state, h_init:clone())\n",
    "end\n",
    "-- training criterion (negative log likelihood)\n",
    "protos.criterion = nn.ClassNLLCriterion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.dot(protos.rnn.fg, 'CharLstm_color', 'CharLstm_color4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CharLstm_color4.svg\" width=\"600\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Parameters for Optimization\n",
    "In Torch one can get a vectorized view of all the parameters and then use these \n",
    "in an optimization algorithm independent of the network structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- put the above things into one flattened parameters tensor\n",
    "params, grad_params = protos.rnn:getParameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "For recureent models, parameter initilaization can be tricky. Here, instead of \n",
    "default Torch initialization using square root of fanin, a uniform distribution is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- initialization\n",
    "params:uniform(-0.08, 0.08) -- small numbers uniform\n",
    "print('number of parameters in the model: ' .. params:nElement())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll in Time\n",
    "As explained above, we now unroll the core a specific number of times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clones = {}\n",
    "for name,proto in pairs(protos) do\n",
    "    print('cloning ' .. name)\n",
    "    clones[name] = crnn.clone_many_times(proto, opt.seq_length)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- evaluate the loss over an entire split\n",
    "function eval_split(split_index, max_batches)\n",
    "    print('evaluating loss over split index ' .. split_index)\n",
    "    local n = loader.split_sizes[split_index]\n",
    "    if max_batches ~= nil then n = math.min(max_batches, n) end\n",
    "\n",
    "    loader:reset_batch_pointer(split_index) -- move batch iteration pointer for this split to front\n",
    "    local loss = 0\n",
    "    local rnn_state = {[0] = init_state}\n",
    "    \n",
    "    for i = 1,n do -- iterate over batches in the split\n",
    "        -- fetch a batch\n",
    "        local x, y = loader:next_batch(split_index)\n",
    "        if opt.gpuid >= 0 then -- ship the input arrays to GPU\n",
    "            -- have to convert to float because integers can't be cuda()'d\n",
    "            x = x:float():cuda()\n",
    "            y = y:float():cuda()\n",
    "        end\n",
    "        -- forward pass\n",
    "        for t=1,opt.seq_length do\n",
    "            clones.rnn[t]:evaluate() -- for dropout proper functioning\n",
    "            local lst = clones.rnn[t]:forward{x[{{}, t}], unpack(rnn_state[t-1])}\n",
    "            rnn_state[t] = {}\n",
    "            for i=1,#init_state do table.insert(rnn_state[t], lst[i]) end\n",
    "            prediction = lst[#lst] \n",
    "            loss = loss + clones.criterion[t]:forward(prediction, y[{{}, t}])\n",
    "        end\n",
    "        -- carry over lstm state\n",
    "        rnn_state[0] = rnn_state[#rnn_state]\n",
    "        print(i .. '/' .. n .. '...')\n",
    "    end\n",
    "\n",
    "    loss = loss / opt.seq_length / n\n",
    "    return loss\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "-- do fwd/bwd and return loss, grad_params\n",
    "local init_state_global = crnn.clone_list(init_state)\n",
    "function feval(x)\n",
    "    if x ~= params then\n",
    "        params:copy(x)\n",
    "    end\n",
    "    grad_params:zero()\n",
    "\n",
    "    ------------------ get minibatch -------------------\n",
    "    local x, y = loader:next_batch(1)\n",
    "    if opt.gpuid >= 0 then -- ship the input arrays to GPU\n",
    "        -- have to convert to float because integers can't be cuda()'d\n",
    "        x = x:float():cuda()\n",
    "        y = y:float():cuda()\n",
    "    end\n",
    "    ------------------- forward pass -------------------\n",
    "    local rnn_state = {[0] = init_state_global}\n",
    "    local predictions = {}           -- softmax outputs\n",
    "    local loss = 0\n",
    "    for t=1,opt.seq_length do\n",
    "        clones.rnn[t]:training() -- make sure we are in correct mode (this is cheap, sets flag)\n",
    "        local lst = clones.rnn[t]:forward{x[{{}, t}], unpack(rnn_state[t-1])}\n",
    "        rnn_state[t] = {}\n",
    "        for i=1,#init_state do table.insert(rnn_state[t], lst[i]) end -- extract the state, without output\n",
    "        predictions[t] = lst[#lst] -- last element is the prediction\n",
    "        loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])\n",
    "    end\n",
    "    loss = loss / opt.seq_length\n",
    "    ------------------ backward pass -------------------\n",
    "    -- initialize gradient at time t to be zeros (there's no influence from future)\n",
    "    local drnn_state = {[opt.seq_length] = crnn.clone_list(init_state, true)} -- true also zeros the clones\n",
    "    for t=opt.seq_length,1,-1 do\n",
    "        -- backprop through loss, and softmax/linear\n",
    "        local doutput_t = clones.criterion[t]:backward(predictions[t], y[{{}, t}])\n",
    "        table.insert(drnn_state[t], doutput_t)\n",
    "        local dlst = clones.rnn[t]:backward({x[{{}, t}], unpack(rnn_state[t-1])}, drnn_state[t])\n",
    "        drnn_state[t-1] = {}\n",
    "        for k,v in pairs(dlst) do\n",
    "            if k > 1 then -- k == 1 is gradient on x, which we dont need\n",
    "                -- note we do k-1 because first item is dembeddings, and then follow the \n",
    "                -- derivatives of the state, starting at index 2. I know...\n",
    "                drnn_state[t-1][k-1] = v\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    ------------------------ misc ----------------------\n",
    "    -- transfer final state to initial state (BPTT)\n",
    "    init_state_global = rnn_state[#rnn_state] -- NOTE: I don't think this needs to be a clone, right?\n",
    "    -- clip gradient element-wise\n",
    "    grad_params:clamp(-opt.grad_clip, opt.grad_clip)\n",
    "    return loss, grad_params\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- start optimization here\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "local optim_state = {learningRate = opt.learning_rate, alpha = opt.decay_rate}\n",
    "local iterations = opt.max_epochs * loader.ntrain\n",
    "local iterations_per_epoch = loader.ntrain\n",
    "local loss0 = nil\n",
    "for i = 1, iterations do\n",
    "    local epoch = i / iterations_per_epoch\n",
    "\n",
    "    local timer = torch.Timer()\n",
    "    local _, loss = optim.rmsprop(feval, params, optim_state)\n",
    "    local time = timer:time().real\n",
    "\n",
    "    local train_loss = loss[1] -- the loss is inside a list, pop it\n",
    "    train_losses[i] = train_loss\n",
    "\n",
    "    -- exponential learning rate decay\n",
    "    if i % loader.ntrain == 0 and opt.learning_rate_decay < 1 then\n",
    "        if epoch >= opt.learning_rate_decay_after then\n",
    "            local decay_factor = opt.learning_rate_decay\n",
    "            optim_state.learningRate = optim_state.learningRate * decay_factor -- decay it\n",
    "            print('decayed learning rate by a factor ' .. decay_factor .. ' to ' .. optim_state.learningRate)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- every now and then or on last iteration\n",
    "    if i % opt.eval_val_every == 0 or i == iterations then\n",
    "        -- evaluate loss on validation data\n",
    "        local val_loss = eval_split(2) -- 2 = validation\n",
    "        val_losses[i] = val_loss\n",
    "\n",
    "        local savefile = string.format('%s/lm_%s_epoch%.2f_%.4f.t7', opt.checkpoint_dir, opt.savefile, epoch, val_loss)\n",
    "        print('saving checkpoint to ' .. savefile)\n",
    "        local checkpoint = {}\n",
    "        checkpoint.protos = protos\n",
    "        checkpoint.opt = opt\n",
    "        checkpoint.train_losses = train_losses\n",
    "        checkpoint.val_loss = val_loss\n",
    "        checkpoint.val_losses = val_losses\n",
    "        checkpoint.i = i\n",
    "        checkpoint.epoch = epoch\n",
    "        checkpoint.vocab = loader.vocab_mapping\n",
    "        torch.save(savefile, checkpoint)\n",
    "    end\n",
    "\n",
    "    if i % opt.print_every == 0 then\n",
    "        print(string.format(\"%d/%d (epoch %.3f), train_loss = %6.8f, grad/param norm = %6.4e, time/batch = %.2fs\", i, iterations, epoch, train_loss, grad_params:norm() / params:norm(), time))\n",
    "    end\n",
    "   \n",
    "    if i % 10 == 0 then collectgarbage() end\n",
    "\n",
    "    -- handle early stopping if things are going really bad\n",
    "    if loss0 == nil then loss0 = loss[1] end\n",
    "    if loss[1] > loss0 * 3 then\n",
    "        print('loss is exploding, aborting.')\n",
    "        break -- halt\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
